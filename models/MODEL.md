`llama-3-Korean-Bllossom-8B-Q8_0.gguf`<br>
위 모델이 속도와 성능 모두 괜찮습니다.

```python
# llama-3.2-Korean-Bllossom-3B
model_path = './models/llama-3.2-Korean-Bllossom-3B/f16.gguf'
model_path = './models/llama-3.2-Korean-Bllossom-3B/Q8_0.gguf'

# llama-3-Korean-Bllossom-8B
model_path = './models/llama-3-Korean-Bllossom-8B/f16.gguf'
model_path = './models/llama-3-Korean-Bllossom-8B/Q8_0.gguf'
model_path = './models/llama-3-Korean-Bllossom-8B/Q4_K_M.gguf'
model_path = './models/llama-3-Korean-Bllossom-8B/Open-Ko Q4_K_M.gguf'

# llama-3-Korean-Bllossom-70B
model_path = './models/llama-3-Korean-Bllossom-70B/Q4_K_M.gguf'

# aya-expanse-8b-abliterated
model_path = './models/aya-expanse-8b-abliterated/Q8_0.gguf'
```
